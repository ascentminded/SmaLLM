{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2By5g5WYhXUq",
        "Llt006H3DUmy"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The purpose of this notebook:\n",
        "\n",
        "The purpose of this notebook is to develop a small scale LLM (a SLM?) of my own, and study the word to word relations uncovered. LLMs have found a lot of use in the study of cognition and speech processing in general, and I think a deeper understanding of the Transformer architecture used in them will help me as a scientist.\n",
        "\n",
        "If that's not your jam, no worries, this is how ChatGPT works, in miniature. This is based on Andrej Karpathy's [GPT from Scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY) video, do check him out on youtube.\n",
        " ***\n",
        "\n",
        "A little mini-goal that I have set for myself is to write better comments, so I hope I achieve that here."
      ],
      "metadata": {
        "id": "2By5g5WYhXUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing"
      ],
      "metadata": {
        "id": "CzqXYrdCi1Us"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RSEzjjzhWOt",
        "outputId": "30582b83-65cb-413d-94b8-485ad3c1a9b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-04 10:42:30--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-12-04 10:42:30 (32.7 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#I am using this doc from Andrej Karpathy, in which all of Shakespeare's works have been concatenated, as my training data\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAvPhSIciiOs",
        "outputId": "244089f7-8025-4e4c-c879-cac4c051619d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text))) #This splits up the above text into individual characters and sorts the unique ones\n",
        "print(\"no. of unique characters is \", len(chars), \"\\nThey are \", chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fDcPlgIi5hl",
        "outputId": "030b3180-37e6-4781-e529-99954445076c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no. of unique characters is  65 \n",
            "They are  ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, apart from the 26 letters, punctuation, spaces, line breaks and a few numbers have also been added to the 'vocabulary' that we will use."
      ],
      "metadata": {
        "id": "CE3QBq5Sj38H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to ***tokenise*** the above characters. Tokenising means converting these characters into integers that representing those characters."
      ],
      "metadata": {
        "id": "n1rnA4mXkP6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below terminology (speech to integer/ s to i) is borrowed from Andrej Karpathy once again, and in this step we initialise two python dictionaries\n",
        "The first makes a dictionary, stoi, which maps each character to its list index (an integer) and another dict, itos which maps the index to the character, allowing decoding and encoding"
      ],
      "metadata": {
        "id": "xZ4BqN-2le3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
      ],
      "metadata": {
        "id": "bz4WD95VkGQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "decode used to be `decode = lambda l: [itos[c] for c in l]`\n",
        "There's an issue with the decoding, though, namely that it doesn't recognise spaces, and would output each letter as a distinct character, not as words.hence the ''.join() command"
      ],
      "metadata": {
        "id": "7zelLNOKmgKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing this out on strings and numbers\n",
        "print(encode(\"test run! one two three\"))\n",
        "print(decode(encode(\"test run! one two three\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwgMkOgSmS0D",
        "outputId": "8deaf545-38d6-4b5f-e215-ad5026995393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[58, 43, 57, 58, 1, 56, 59, 52, 2, 1, 53, 52, 43, 1, 58, 61, 53, 1, 58, 46, 56, 43, 43]\n",
            "test run! one two three\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we encode the source text."
      ],
      "metadata": {
        "id": "RHvJrQ3hq-tM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # we use PyTorch: https://pytorch.org, a very handy package for deep learning in general.\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "jn1fQLSHq-SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Something fun to note, is that the real tokenisers, as used in GPT and so forth, don't work on a character level.\n",
        "They use syllables, sections of words, entire words, etc. So the no. of distinct tokens in GPT2 is around 50,000!"
      ],
      "metadata": {
        "id": "loUa4VJ1nNtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train-test split (seperating out some data that is unfamiliar to the model, that we can fairly test the data on)\n",
        "split_pt = int(0.8 * len(data)) #the int is important, because you can't index using a float variable (Imagine asking, \"what's the 37.6th item in your list?\")\n",
        "test =  data[split_pt:]\n",
        "train = data[:split_pt]"
      ],
      "metadata": {
        "id": "wKn2f2jEnjZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a deep network is never done in one go. Instead of one gulp, the data is fed into the transformer in bite sized chunks. There can be multiple bites at a time, though (the metaphor is getting a little confusing at this point).\n",
        "\n",
        "Essentially, instead of asking the model to learn all the connections between the words in the entirety of the data, it is tasked with studying what comes next after one word, two words, or so. It learns in small sentences. However, multiple small sentences can be processed simultaneously without interference.\n",
        "\n",
        "\"The maximum context length for the prediction\" is given by how big the bite is.\n",
        "\n",
        "GPUs are excellent at parallel processing, and it helps drastically cut down on training and testing time. One appealing thing about transformer networks, and a lot of DL models, is that it is essentially a number of matrix multiplications, which are easily parallelised.\n",
        "\n",
        "As such, we use two variables 'block size' and 'batch size' to show how big each chunk of data is, and how many chunks of data are fed into the transformer simultaneously"
      ],
      "metadata": {
        "id": "X8a9YYOeoTgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337) #The 1337 is a pun, leet, which tells you how old Andrej is. More importantly, setting a 'seed' makes sure that the same 'random' number is generated each time you run this code\n",
        "block_size = 8\n",
        "batch_size = 4\n",
        "\n",
        "def batching(split):\n",
        "  data = train if split=='train' else test #If i run batching(train) it batches the train_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,)) #This is a random integer less than len(data)- block size\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])#these are the targets, basically the next character for each data point in x\n",
        "  return x, y\n",
        "xb, yb = batching('train')"
      ],
      "metadata": {
        "id": "I2ZOxHAIpYxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) #This is our input to the transformer, 4 chunks of 8 charas each\n",
        "print(\"\\n\",yb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_x14SDkhrTe7",
        "outputId": "98246446-3d6a-4faf-c314-ec31f154d81b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[58, 63,  8,  0,  0, 19, 24, 27],\n",
            "        [39, 59, 45, 46, 58,  1, 46, 43],\n",
            "        [49, 43, 57,  1, 53, 50, 42,  1],\n",
            "        [52, 41, 47, 43, 52, 58,  1, 56]])\n",
            "\n",
            " tensor([[63,  8,  0,  0, 19, 24, 27, 33],\n",
            "        [59, 45, 46, 58,  1, 46, 43,  1],\n",
            "        [43, 57,  1, 53, 50, 42,  1, 46],\n",
            "        [41, 47, 43, 52, 58,  1, 56, 47]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The basic structure of the model\n",
        "For this model, we're building our deep learning model, one of"
      ],
      "metadata": {
        "id": "1cHrXSfBvJ0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn #Essential for building neural nets in torch\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "O7kior22Vzsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The basic structure of the single character, no attention, model that we're building goes like this:\n",
        "```\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__() #this bit inherits the characteristics of nn.Module\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) #https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
        "  def forward(self,idx, targets=None):\n",
        "    logits = self.token_embedding_table(idx) #reads off probabilities/logits of next token from the embedding table\n",
        "    loss = F.cross_entropy()\n",
        "\n",
        "    return loss,logits\n",
        "```\n",
        "However, in the actual code, we need to permute and rearrange the elements in the tensor, because forward (nn.Embedding) requires something of the form (minibatch_no,channel number(from vocab_size),etc), while by default we have (batch,time,channel). So we're ensuring that channel remains in the second dimension by turning batch,time into one extended dim.\n"
      ],
      "metadata": {
        "id": "SaJJ-0vFCz2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__() #this bit inherits the characteristics of nn.Module\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) #https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
        "\n",
        "  def forward(self,idx, targets=None):#targets is None, optional, to determine if it's train or test, else when we call forward from generate, there will be an error due to lack of targets\n",
        "    logits = self.token_embedding_table(idx) #reads off probabilities/logits of next token from the embedding table\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "          #batch no, timept, channel no. See above text note for explanation\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits,loss\n",
        "\n",
        "  def generate(self,idx,max_new_tokens):\n",
        "    #max_new_tokens is the number of new characters/tokens to be generated after the input\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits,loss = self.forward(idx)\n",
        "      logits = logits[:,-1,:]#last element in the time dimension, most recent timepoint\n",
        "      probs = F.softmax(logits, dim=-1) #(B,c)\n",
        "      idx_next = torch.multinomial(probs,num_samples=1) #(B,1)\n",
        "      idx = torch.cat((idx,idx_next),dim=1) #(B,T+1)\n",
        "    return idx\n"
      ],
      "metadata": {
        "id": "hcqCZDYVwDpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = BigramLanguageModel(vocab_size=len(char))\n",
        "logits,loss= m(xb,yb)\n",
        "print(logits.shape)\n",
        "print(\"The loss is \",loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7tHFO0CEfjO",
        "outputId": "688d7d6b-453d-4bc8-92bf-30b8cb999420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "The loss is  tensor(4.6627, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the loss is sheer luck, it's -log(likelihood), -ln(1/len(char)), or 4.17 and our current loss is a little higher."
      ],
      "metadata": {
        "id": "tqvKdjlCFNFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Generating new tokens\n",
        "idx = torch.zeros((1,1),dtype=torch.long) #Starting with a new line makes sense, and encode(\"\\n\") gives [0]\n",
        "print(decode(m.generate(idx,max_new_tokens=1000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8zi6E2vHs0f",
        "outputId": "31d421e5-9589-4f5d-93de-693dd93948a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "l-QYjt'CL?jLDuQcLzy'RIo;'KdhpV\n",
            "vLixa,nswYZwLEPS'ptIZqOZJ$CA$zy-QTkeMk x.gQSFCLg!iW3fO!3DGXAqTsq3pdgq!LznIeJydZJSrFSrPLR!:VwWSmFNxbjPiNYQ:sry,OfKrxfvJI$WS3JqCbB-TSQXeKroeZfPL&,:opkl;Bvtz$LmOMyDjxxaZWtpv,OxZQsWZalk'uxajqgoSXAWt'e.Q$.lE-aV\n",
            ";spkRHcpkdot:u'-NGEzkMPy'hZCWhv.w.q!f'mOxF&IDRR,x\n",
            "?$Ox?xj.BHJsGhwVtcuyoMIRfhoPL&fg-NwJmOQalcEDveP$IYUMv&JMHkzd:O;yXCV?wy.RRyMys-fg;kHOB EacboP g;txxfPL\n",
            "NTMlX'FNYcpkHSGHNuoKXe..ehnsarggGFrSjIr!SXJ?KeMl!.?,MlbDP!sfyfBPeNqwjLtIxiwDDjSJzydFm$CfhqkCe,n:kyRBubVbxdojhEzAtV\n",
            "l;Undhmj.KZaOZJnHlrAaAQcn-iugqTxJ;Ig,NqE&HOxzYcLyHaxyj'ak'StIhPBfJi3Y.uFYc$'NqtvDXhot;tXacKz$FU&V.bESfOng;;:N\n",
            "OoeAgkcLo'dF&$ydutvA$VrIJdTkBHcb-T itZmY&qEh;lg\n",
            "\n",
            "O;kHQYQNCd yeXhfUOm FvDmVehVerKkDF bv3pPyXAg;ukn:OajcSl;.kHF3Ml?llX\n",
            "xVtIrK-kHE;:sZElrIZ\n",
            "tTx-wBPfqTgLNcCy,abjxFg;tVxFdFlpdoimjDRSJs&UPL?$kas:uvg\n",
            "k!kDptzkcusoCZJ afBkAs:Naj'aIUrXN!LwJIY..kjwFfduwRjfvsroed.iuHg.WFEhyC?t:WtkBfGTZKjfh;&tkwFRkNsThFj.eJ rCoh3,OxjtffUPGXyTTC$m Xcn?q$CGukHMXnAQTwNUzmZED3fmOEX\n",
            "fJOdFvLwC.Q!Np'&fhTHrbOrCC IRTyprPe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the text is gibberish. Now it's time to train the model. In order to train a model, you need an optimiser, like Standard Gradient Descent (SGD), or the most common and well used optimiser, Adam (an abbreviation of Adaptive something or other)\n",
        "\n"
      ],
      "metadata": {
        "id": "3Le3gOVYFrWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Optimiser loop\n",
        "batch_size = 32\n",
        "optimizer = torch.optim.Adam(m.parameters(),lr=1e-3) #The learning rate of 1e-3 since this is a very basic model,  3e-4 is a better standard protocol\n",
        "for steps in range(10000):\n",
        "  xb,yb = batching(train)\n",
        "  _,loss = m(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True) #At each step, the optimiser doesn't start with any value of gradient (Look up https://builtin.com/data-science/gradient-descent)\n",
        "  loss.backward()#calculates the gradient(loss) using backpropagation\n",
        "  optimizer.step()#changes parameters in the opposite direction of gradient, so that loss decreases\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1uikfp4Fqyz",
        "outputId": "17f0dd1a-13f5-4e62-9ee4-546b319aafe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4418282508850098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx,max_new_tokens=1000)[0].tolist()))"
      ],
      "metadata": {
        "id": "w1TA1PB4OZKM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8470614e-a688-406a-8608-b8773b1c4b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "n.\n",
            "Paure wagepplirn!\n",
            "meftou ow prink, avewist th;thomayo alingienco, An he ware whiougou he s imaror?\n",
            "Bu ne-ithof acat: bel,\n",
            "Fothind at wrt:\n",
            "HQUCANTh ros!\n",
            "ANQNThes med thestw cos wand herrs yorfold madlous jouney biPoer bngabtestouMatswo IONThathery ththe tonty th, fourid irtys ndyod pp qur awe ainowhemy azur:\n",
            "I,\n",
            "Ishit tinghast ha tteredef seasiomamy.\n",
            "Makine,\n",
            "\n",
            "TISARThe ientr?\n",
            "GRO:\n",
            "DELAre y l, ure t codausioftotierr,\n",
            "her tr fed\n",
            "th h'er?\n",
            "Sh woo!qual,\n",
            "A y kise ugive n telo,\n",
            "ANCEThisk, thoa iroro s lly ndst onindave S:\n",
            "\n",
            "me; d monderecr.\n",
            "UCofre,\n",
            "Pe lllit, ddff; choManome ureswir anqur t h sele s wame me Jo mshe 'ro'e s terena h w towant ak.\n",
            "KEMIRSe dorome hels, to aly f t, mmelel meauns athed fr yeak, LELUMo tce be e hend VIO:\n",
            "Whell a t te ghs: t wed.\n",
            "Thintsheshuine,\n",
            "'sowathepon.\n",
            "\n",
            "ENTO:\n",
            "LOMI I sethethor, l f ghato, wn nz'SI tar\n",
            "Thohor llo:\n",
            "PedHAPENCHWee are, PELirr tyowail I towithengory t ce stheer.\n",
            "\n",
            "yo awes ntherem; ssted bee iteme ur,\n",
            "TMIAd cou pe Lof IORTams webear, mss be'Zxpans! alis\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running it for several steps, few epochs drops the loss, giving us a more coherent gibberish"
      ],
      "metadata": {
        "id": "G4kHNH7TOJxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-Attention"
      ],
      "metadata": {
        "id": "Llt006H3DUmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self attention basically builds in connections between different tokens at each step."
      ],
      "metadata": {
        "id": "u3Lv8uBTOh1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn #Essential for building neural nets in torch\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "rTwxUNdBKMdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qTEv6DyPOEr",
        "outputId": "7c550963-6a94-4835-e6c6-d210375cff67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "print(wei)\n",
        "xbow3 = wei @ x #weighted matrix * our matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1o8ByWU_OvqM",
        "outputId": "cb7e7213-5442-41f3-fe28-a0b13859aeca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, using a lower triangle matrix of ones (torch.tril()), filling 0s with -infs, (this shows that at each stage, the subsequent tokens are of negligible priority), and then using softmax to generate values that sum to one at each row, we basically create the contribution of each token to the prediction of the next."
      ],
      "metadata": {
        "id": "tnJ2eVJbPTVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notes from Andrej Karpathy:**\n",
        "\n",
        "Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "\"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "\"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ],
      "metadata": {
        "id": "IiKegWOiWdur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "head_size = 16\n",
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1)\n",
        "print(\"variance of q: \",q.var(),\" k: \",k.var())\n",
        "print(\"variance of wei without normalising: \",wei.var())\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5\n",
        "print(\"variance of wei after normalising:\", wei.var())\n",
        "print(\"Wei feeds into a softmax, and during initialisationm if wei takes on very positive or very negative numbers, it tends to form one-hot vectors - it would peak at the values with greatest variation\")"
      ],
      "metadata": {
        "id": "ehTl9vMOPtvK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48ca10e9-9d69-4503-91dd-bf82b87105ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "variance of q:  tensor(1.0449)  k:  tensor(1.0554)\n",
            "variance of wei without normalising:  tensor(18.1082)\n",
            "variance of wei after normalising: tensor(1.1318)\n",
            "Wei feeds into a softmax, and during initialisationm if wei takes on very positive or very negative numbers, it tends to form one-hot vectors - it would peak at the values with greatest variation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the model, component by component"
      ],
      "metadata": {
        "id": "WDXxpVY2BRCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "vocab_size = len(char)\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3 #Self-Attention can't tolerate very high learning rates\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0"
      ],
      "metadata": {
        "id": "RfzjXGvwA9ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Self attention**\n",
        "- Self attention here is built not on the average that we built earlier, using softmax, but allowing for unequal contributions, each token would have different context cues after all (vowels, consonants)\n",
        "- In this self-attention model, each token at each position is assigned two vectors: a *key* and a *query*. These represent what they 'contain' and what they 'look for' respectively\n",
        "- The weighted avg vector, wei, is replaced by the dot product of a token's query with all the previous tokens' keys. (and then the previous processes and softmax are done as normal)\n",
        "- If a key and query are aligned, the dot product will be higher than otherwise, and the weight of that key will be higher"
      ],
      "metadata": {
        "id": "mSLzFcA5s9ng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C) #random matrix of shape (B,T,C)\n",
        "\n",
        "#Self-Attention Head:\n",
        "head_size = 16\n",
        "key = nn.Linear(C,head_size, bias=False) #bias is typically not used when it's not a trainable parameter (when there's a layer after it that pools, etc and cancels it out)\n",
        "query = nn.Linear(C, head_size,bias = False)\n",
        "value = nn.Linear(C, head_size , bias = False)\n",
        "\n",
        "k = key(x) #(B,T,16/head_size)\n",
        "q = query(x) #(B,T,16)\n",
        "v = value(x)\n",
        "wei = q @ k.transpose(-2,-1) #(B,T,16) @ (B,16,T) --> (B,T,T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T)) #Because same for each batch\n",
        "wei = wei.masked_fill(tril==0,float('-inf'))#Remove this if nodes from the present need context from the future : e.g. sentiment analysis encoders\n",
        "wei = F.softmax(wei,dim=-1)\n",
        "#out = wei @ x\n",
        "out = wei @ v #\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "atGe1MAubYKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "print(out.shape)\n",
        "print(wei[0])\n",
        "print(\"\\nThis is clearly from the untrained values of weights, but they're definitely disproportionate, not equal values\")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ItHsdGaG0Cx8",
        "outputId": "addabc12-42e1-4409-e7eb-2681e3fe488c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(out.shape)\\nprint(wei[0])\\nprint(\"\\nThis is clearly from the untrained values of weights, but they\\'re definitely disproportionate, not equal values\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A single head of attention**\n",
        "- We start with a single 'head' of attention"
      ],
      "metadata": {
        "id": "GscGine-qJnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  \"\"\"one head of attention\"\"\"\n",
        "  def __init__(self,head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd,head_size, bias=False) #bias is typically not used when it's not a trainable parameter (when there's a layer after it that pools, etc and cancels it out)\n",
        "    self.query = nn.Linear(n_embd, head_size,bias = False)\n",
        "    self.value = nn.Linear(n_embd, head_size , bias = False)\n",
        "    self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size))) #This replaces the tril that was initialised separately, see he masked fill line now\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x) #(B,T,C)\n",
        "    q = self.query(x) #(B,T,C/head_size)\n",
        "\n",
        "    #compute attention scores, aka 'affinities'\n",
        "    wei = q @ k.transpose(-2,-1) * C **(-0.5)\n",
        "    tril = torch.tril(torch.ones(T,T)) #Because same for each batch\n",
        "    wei = wei.masked_fill(self.tril[:T,:T]==0,float('-inf')) #Remove this if nodes from the present need context from the future : e.g. sentiment analysis encoders\n",
        "    wei = F.softmax(wei,dim=-1)\n",
        "    wei = self.dropout(wei)\n",
        "\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "-M5l6Pmzbiep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-Headed Attention**\n",
        "- Now we add multi-headed attention, which basically means we add multiple heads (like the ones we created above) in parallel, like so:"
      ],
      "metadata": {
        "id": "iW2lWR7ySux0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "  def forward(self,x):\n",
        "    return torch.cat([h(x) for h in self.heads], dim = -1)"
      ],
      "metadata": {
        "id": "zjEwL6-mStvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feedforward Block**\n",
        "- The feedforward block is simply a multi-layer perceptron or MLP (nn.Linear)\n",
        "- Here we use a single linear layer from n_embd nodes to n_embd nodes, with a ReLU activation function\n",
        "- According to Andrej, this essentially is an opportunity to 'think on' the data developed during the interconnected knowledge transfer from the attention block"
      ],
      "metadata": {
        "id": "B8BthNRCnaPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,n_embd):\n",
        "    super().__init__()\n",
        "    self.ffwd = nn.Sequential(\n",
        "        nn.Linear(n_embd,n_embd),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.ffwd(x)"
      ],
      "metadata": {
        "id": "E7tvFSy3nZ13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block arrangement**\n",
        "- Blocks of communication and computations (embedding tables & attention, and MLPs) alternately arranged make up the structure of the Transformer"
      ],
      "metadata": {
        "id": "SlNLL8SypG6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self,n_embd,n_heads):\n",
        "    super().__init__()\n",
        "    head_size = n_embd//n_heads\n",
        "    self.sa = MultiHeadAttention(n_heads,head_size)\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.sa(x)\n",
        "    x = self.ffwd(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "emCoz-SwpWyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Skip connections and norm**\n",
        "- With these blocks of attention blocks, MLPs and more building up, the deep neural network is actually becoming fairly, well, deep.\n",
        "- The deeper a network gets, it runs into issues of overfitting and potentially also becoming very exorbitant to train\n",
        "- In this case, we use two tricks:\n",
        "    - **Residual Connections aka Skip Connections**\n",
        "      - Source: [Deep Residual Learning for Image Recognition - He et. al, 2015](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)\n",
        "      - There is addition from the source, i.e. if the deep net takes a node A and performs some computations on it\n",
        "      - Say A -> ReLU(Linear(Attention(A))) -> B\n",
        "      - A 'residual' or skip connection straight from A -> B can be made, based solely on +\n",
        "      - The end result basically is ReLU(Lin(Att(A))) + A\n",
        "      - Gradients are equally distributed during sums, so gradients of the loss are forked off ?? Yeah I'm lost\n",
        "      - Each block then comes online slowly during optimization (This is also beyond my current understanding)\n",
        "    - **Layer Norm**\n",
        "      - Layer normalisation is very similar to batch normalisation, which ensures that across the batch dimension, each neuron had gaussian distribution (0 mean and 1 std deviation)\n",
        "      - Layer norm, however, doesn't normalise the columns, it normalises the rows.\n",
        "      - Another thing to note: since the publication of the *Attention is all you need* paper, not much has changed when it comes to transformer structure. However, nowadays layer normalisation is done prior to the attention blocks rather than afterwards (pre-norm)."
      ],
      "metadata": {
        "id": "dC-wBw0Eq4-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final (complete) model"
      ],
      "metadata": {
        "id": "uGf1MWzvO1KK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "#updated hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 3e-4 #Self-Attention can't tolerate very high learning rates\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "9kzzx4dENZ3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "JyPgK5bD0EdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  \"\"\"one head of attention\"\"\"\n",
        "  def __init__(self,head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd,head_size, bias=False) #bias is typically not used when it's not a trainable parameter (when there's a layer after it that pools, etc and cancels it out)\n",
        "    self.query = nn.Linear(n_embd, head_size,bias = False)\n",
        "    self.value = nn.Linear(n_embd, head_size , bias = False)\n",
        "    self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size))) #This replaces the tril that was initialised separately, see he masked fill line now\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x) #(B,T,C)\n",
        "    q = self.query(x) #(B,T,C/head_size)\n",
        "\n",
        "    #compute attention scores, aka 'affinities'\n",
        "    wei = q @ k.transpose(-2,-1) * C **(-0.5)\n",
        "    tril = torch.tril(torch.ones(T,T)) #Because same for each batch\n",
        "    wei = wei.masked_fill(self.tril[:T,:T]==0,float('-inf')) #Remove this if nodes from the present need context from the future : e.g. sentiment analysis encoders\n",
        "    wei = F.softmax(wei,dim=-1)\n",
        "    wei = self.dropout(wei)\n",
        "\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "1_arQTQdQnQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we implement these skip connections into the classes, # indicates new lines\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd,n_embd)  ##\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "    out = self.proj(out)  ##\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,n_embd):\n",
        "    super().__init__()\n",
        "    self.ffwd = nn.Sequential(\n",
        "        nn.Linear(n_embd,4 * n_embd), #The 4 * n_embd is taken directly from the attention paper, where the inner layer of ffwd is 4 times input\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embd,n_embd), ##\n",
        "        nn.Dropout(dropout), ##\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.ffwd(x)"
      ],
      "metadata": {
        "id": "w4RINE4MLHXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Updated block\n",
        "class Block(nn.Module):\n",
        "  def __init__(self,n_embd,n_heads):\n",
        "    super().__init__()\n",
        "    head_size = n_embd//n_heads\n",
        "    self.sa = MultiHeadAttention(n_heads,head_size)\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd) #Normalises across to token, but is also optimisable\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "bkBsGJNAsySw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The cooler model, now incorporating the above 'Head' of attention\n",
        "\n",
        "\n",
        "class SmallLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size,n_embd)\n",
        "        #self.sa_head = Head(n_embd) #For single head attention\n",
        "        self.sa_heads = MultiHeadAttention(n_head, n_embd//n_head) #n_heads being four, 4 heads of 8-dimensional attention. // takes the floor of the quotient\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        '''self.blocks = nn.Sequential(\n",
        "            Block(n_embd,n_heads=4),\n",
        "            Block(n_embd,n_heads=4),\n",
        "            Block(n_embd,n_heads=4),\n",
        "            nn.LayerNorm(n_embd),\n",
        "        )'''#replaced by the next chunk\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_heads=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd,vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        B,T = idx.shape\n",
        "\n",
        "        tok_embed = self.token_embedding_table(idx)\n",
        "        pos_embed = self.position_embedding_table(torch.arange(T,device=device))\n",
        "        embed = tok_embed + pos_embed\n",
        "        embed = self.sa_heads(embed)\n",
        "        embed = self.ffwd(embed)\n",
        "        embed = self.blocks(embed)\n",
        "        embed = self.ln_f(embed) # (B,T,C)\n",
        "        logits = self.lm_head(embed) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_crop = idx[:,-block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_crop)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "SSrHZZqAA1CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SmallLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "#max_iters = 5000\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "P-Lw1kpkA_cv",
        "outputId": "7e75bb41-be42-4341-f9b2-9f3a9cadd389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.560705 M parameters\n",
            "step 0: train loss 4.3500, val loss 4.3486\n",
            "step 100: train loss 3.0778, val loss 3.1211\n",
            "step 200: train loss 2.5301, val loss 2.5234\n",
            "step 300: train loss 2.4576, val loss 2.4628\n",
            "step 400: train loss 2.3926, val loss 2.4201\n",
            "step 500: train loss 2.2683, val loss 2.3069\n",
            "step 600: train loss 2.0914, val loss 2.1430\n",
            "step 700: train loss 1.9197, val loss 2.0114\n",
            "step 800: train loss 1.7689, val loss 1.9071\n",
            "step 900: train loss 1.6778, val loss 1.8550\n",
            "step 1000: train loss 1.5946, val loss 1.7880\n",
            "step 1100: train loss 1.5311, val loss 1.7306\n",
            "step 1200: train loss 1.4908, val loss 1.6971\n",
            "step 1300: train loss 1.4483, val loss 1.6569\n",
            "step 1400: train loss 1.4127, val loss 1.6333\n",
            "step 1500: train loss 1.3887, val loss 1.6104\n",
            "step 1600: train loss 1.3621, val loss 1.5834\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-8322a52bdef7>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# every once in a while evaluate the loss on train and val sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_iters\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-54b9c0a0b4bb>\u001b[0m in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpt4737DZPCj",
        "outputId": "60d28626-8142-4692-b180-02d094e38bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "HASTINGS:\n",
            "How, Thy granding of thy biock have nought?\n",
            "\n",
            "GLOUCESTER:\n",
            "This crown of thy spirit\n",
            "He or back end of powern your honour's breds:\n",
            "Ere buid his his siders' smonutunt power\n",
            "And as out once must and seal down me more.\n",
            "\n",
            "ANGELO:\n",
            "A pherdic no, name take me sleep.\n",
            "\n",
            "SICINIUS:\n",
            "As it was, lest, by be spite: 'tis is no flesh.\n",
            "\n",
            "PARIS:\n",
            "How now, my might to world, and is to off the land?\n",
            "\n",
            "RATCLIF:\n",
            "And the glory upon him.\n",
            "\n",
            "MARIANA:\n",
            "By all; and thinks and, here did my name.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Not I thanks, what would I do himself have word,\n",
            "He had you ever mistress, gentleman me\n",
            "The cursely of speck, and so leave wise and easure.\n",
            "Call we must I see a foot of me;\n",
            "And give answer all to her insto wife,\n",
            "That shall nor with a saf pather's ere lay,\n",
            "Which my brother's for my kindle flear;\n",
            "The toold hour eyes daughter's watching all,\n",
            "And my comfors of his is thing sound's from myself.\n",
            "Did driven is speak streetch well!\n",
            "\n",
            "GREEN:\n",
            "Ten his love, lawbury. I disgrace; and of ild you king\n",
            "The eye steet no this own apost,\n",
            "And with a man of winders or mine him;\n",
            "You half I descond him, he love and is me.\n",
            "\n",
            "CORIOLANUS:\n",
            "I immore undertake you than to me?\n",
            "\n",
            "JULY:\n",
            "He, sir, warm ubl suffected not duke falsed himself!\n",
            "Some Petia first.\n",
            "\n",
            "First Senator:\n",
            "I think this power; and despoked distrows.\n",
            "\n",
            "LUCIO:\n",
            "Comme, for his like with our, I thank me, my father,\n",
            "I will I shall your see\n",
            "Erge appeace offence any man his,\n",
            "So she readings gates, till pendure at a mine of yours,\n",
            "And they have been her you have will you:\n",
            "And he did them to her leave me I am.\n",
            "\n",
            "EDWARD:\n",
            "How none with me? my masters and tread. an thine sleep\n",
            "ruises to be trive; and it was bove gings,\n",
            "A than make his shall be thing firm of his life,\n",
            "And if my honourable shore a eye: I will\n",
            "Call misering them: go then reves to give.\n",
            "\n",
            "BRUTUS:\n",
            "I he not a most be; and I pagi not, and I must\n",
            "I had let a beheld a country a such a gafter:\n",
            "Hadly with I convent and caured to me\n",
            "By my due mister'd face thing revenge\n",
            "It in good of the lod of his rice rescured.\n",
            "But,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, this isn't exactly GPT, not just because of the scale (we have around 12million parameters, GPT has billions), but also because this decoder is equivalent to the pre-training stage of GPT.\n",
        "OpenAI went on to train GPT using supervised learning, showing thousands of prompts and labels, indicating how a chat assistant should answer. Otherwise, it was likely to barf out text that matched what it learned on the internet - answering questions with questions, links, entire articles, or so on."
      ],
      "metadata": {
        "id": "zYkZC6FDTjDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regardless, I think this has been a very fun learing experience.\n",
        "I hope the section-by-section elaboration that I've written out has been helpful.\n",
        "Once again, I do have a primary source for this, and it's Andrej Karpathy's ['GPT from scratch' video](/https://www.youtube.com/watch?v=kCc8FmEb1nY). If you're not familiar with his work, I definitely recommend you check him out, he has very thorough tutorials and explanations about deep learning and AI, just like this one."
      ],
      "metadata": {
        "id": "z3YULiGpUNRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Good luck, happy Transforming! :)\")"
      ],
      "metadata": {
        "id": "j3kNzxP3UyXb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}