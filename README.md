I am currently expanding on my knowledge of deep learning by tinkering with transformer architectures.
So far, I've used the famous paper _Attention is all you need_ , workshops and tutorials to build a small custom LLM, Shakespeare_LLM.
Soon, I intend to expand on this, and when it's polished, I'll update this repo
